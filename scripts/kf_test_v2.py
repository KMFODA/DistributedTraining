import torch.multiprocessing as mp
import torch
import time
from mapreduce import Peer
import bittensor as bt
from argparse import ArgumentParser
from template.base.neuron import BaseNeuron
from transformers import AutoModelForCausalLM, AutoTokenizer
from template.data.dataset import SubsetFalconLoader
import random

# python3 map-reduce-subnet/test/test.py --netuid 32 --validator.uid 0 --wallet.name default --wallet.hotkey default --subtensor.network test
# tensor size for testing, set to 10 MB
tensor_size = 68 * 1024 * 1024
bandwidth = tensor_size * 4  # torch.float32 is 4 bytes

parser = ArgumentParser()
parser.add_argument("--netuid", type=int, help="Network netuid", default=25)

bt.wallet.add_args(parser)
bt.subtensor.add_args(parser)
bt.axon.add_args(parser)

config = BaseNeuron.config()
config.wallet.name = "test_dt"
config.wallet.hotkey = "validator_1"
config.netuid = 25
config.axon.port = 22043

wallet = bt.wallet(config=config)
subtensor = bt.subtensor(config=config)
metagraph = subtensor.metagraph(config.netuid)

# Init device
device = config.neuron.device

# Init Model
model = AutoModelForCausalLM.from_pretrained(config.neuron.model_name)

# Move the model to the appropriate device
model = model.to(device)

# Set up a decentralized optimizer that will average with peers in background
opt = torch.optim.AdamW(model.parameters(), lr=config.neuron.lr)

tokenizer = AutoTokenizer.from_pretrained(config.neuron.model_name)
# Add the EOS token as PAD token to ensure our dataloader doesn't throw an error for sequences of unequal length
tokenizer.pad_token = tokenizer.eos_token

dataloader = SubsetFalconLoader(
    batch_size=config.neuron.local_batch_size_train,
    sequence_length=1024,
    rows=random.choices(range(0, 968000015), k=1),
)

parser_2 = ArgumentParser()
parser_2.add_argument("--port.range", default="22043:22049", help="Opened Port range")
parser_2.add_argument("--validator.uid", type=int, default=0, help="Validator UID")
parser_2.add_argument("--netuid", type=int, default=32, help="Subnet UID")
parser_2.add_argument(
    "--subtensor.network", type=str, help="Network netuid", default="test"
)

bt.subtensor.add_args(parser_2)
bt.logging.add_args(parser_2)
bt.wallet.add_args(parser_2)
bt.axon.add_args(parser_2)
config_2 = bt.config(parser_2)

weights = torch.rand((tensor_size, 1), dtype=torch.float32)
weights = [torch.tensor(layer) for layer in model.parameters()]
# breakpoint()

for i, batch in enumerate(dataloader):
    inputs = batch.to(device)
    # inputs = batch

    # Forward pass
    outputs = model(input_ids=inputs, labels=inputs)

    # loss = outputs.loss / config.neuron.local_batch_size_train_total  # Scale loss
    loss = outputs.loss
    loss.backward()
    bt.logging.success(f"ðŸŸ¢ Step: {i}")
    bt.logging.success(f"ðŸŸ¢ Loss: {loss}")


# breakpoint()
def train(rank, peer_count, bandwidth):
    bt.logging.info(f"ðŸ”· Starting peer with rank {rank}")
    # Initialize Peer instance
    peer = Peer(rank, peer_count, config=config_2, bandwidth=bandwidth)

    # Initialize process group with the fetched configuration
    peer.init_process_group()

    # weights = None

    # if rank == 1: # if it is the first peer
    #     weights = torch.rand((tensor_size, 1), dtype=torch.float32)
    #     # weights = [torch.tensor(layer) for layer in model.parameters()]
    #     # First peer broadcasts the weights
    #     peer.broadcast(weights)
    # else:
    #     # Other peers receive the weights
    #     weights = peer.broadcast(weights)

    # # Should destroy process group after broadcasting
    # peer.destroy_process_group()

    # Number of epochs
    epoch = 2

    # Your training loop here
    bt.logging.info(f"Peer {rank} is training...")

    for i in range(epoch):
        bt.logging.success(f"ðŸŸ¢ Epoch: {i}")

        # Replace this with actual training code

        # After calculating gradients
        gradients = torch.ones((tensor_size, 1), dtype=torch.float32)
        gradients = [torch.tensor(layer.grad).to("cpu") for layer in model.parameters()]

        if rank == 1:
            #     # gradients = torch.ones((tensor_size, 1), dtype=torch.float32) * 3
            gradients = gradients * 3

        # Initialize process group
        peer.init_process_group()

        # All-reducing the gradients (average of gradients)
        gradients = peer.all_reduce(gradients)

        # Destroy process group
        peer.destroy_process_group()

    bt.logging.success(f"Peer {rank} has finished training.")


def main():
    peer_count = 3
    processes = []

    # Start two peer processes
    for rank in range(1, peer_count + 1):
        p = mp.Process(target=train, args=(rank, peer_count, tensor_size))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()


if __name__ == "__main__":
    # mp.set_start_method('spawn')  # This is often necessary in PyTorch multiprocessing
    main()
